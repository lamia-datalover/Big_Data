{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b067d1cd-5830-4d72-91b3-ea5ddebf36c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The goal of this manipulation is to get familiar with PySpark RDDs , if you have ever heard of \"Hello world\" in web development , \"Word Count\"\n",
    "# is the same in distributed computing \n",
    "# In this notebook, I will setup a pipeline that will count the words in a document in a distributed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d66f9879-c191-4b94-bead-2d45c724d306",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Defining the spark context to play with RDDs\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a365ca2-128e-4167-8d56-8896a3d20145",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# S3 filepath\n",
    "FILENAME = 's3://full-stack-bigdata-datasets/Big_Data/purple_rain.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cdd896b-f756-4c2e-81a2-0a99599d5668",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Loading the filepath to a Spark RDD \n",
    "text_file = sc.textFile(FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194e29d0-be88-472b-8abf-da3245a5bbf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: s3://full-stack-bigdata-datasets/Big_Data/purple_rain.txt MapPartitionsRDD[4] at textFile at NativeMethodAccessorImpl.java:0"
     ]
    }
   ],
   "source": [
    "text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f070f700-b89e-4536-92de-c1696d36f44a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: ['I never meant to cause you any sorrow',\n 'I never meant to cause you any pain',\n 'I only wanted one time to see you laughing']"
     ]
    }
   ],
   "source": [
    "# The last cell doesn't tell us much, let's check the first 3 elements of this RDD\n",
    "text_file.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1835d9-33d5-4ea2-861a-cba5bcc365f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of tokens\n",
    "tokens = text_file.flatMap(lambda line: line.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f280de-58e2-4751-b1c1-e2ba15f75629",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: ['I', 'never', 'meant', 'to', 'cause', 'you', 'any', 'sorrow', 'I', 'never']"
     ]
    }
   ],
   "source": [
    "tokens.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5bb8d38-2d5f-4ff6-8597-f233e3621850",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: [('I', 1),\n ('never', 1),\n ('meant', 1),\n ('to', 1),\n ('cause', 1),\n ('you', 1),\n ('any', 1),\n ('sorrow', 1),\n ('I', 1),\n ('never', 1)]"
     ]
    }
   ],
   "source": [
    "# Now that we have our list of words (well, not exactly a list of words, it is still a RDD), we can start counting things.we need to map each word to an initial count\n",
    "# Writing a function that takes a token as input and returns a tuple then mapping the tokens to this function to create the variable partial_count\n",
    "def token_to_tuple(token):\n",
    "    return (token, 1)\n",
    "partial_count = tokens.map(token_to_tuple)\n",
    "partial_count\n",
    "partial_count.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cbab134-cf1a-47f3-a410-ddc85f5c6ed4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: [('never', <pyspark.resultiterable.ResultIterable at 0x7fa8cb72eb20>),\n ('cause', <pyspark.resultiterable.ResultIterable at 0x7fa8cb72ee50>),\n ('pain', <pyspark.resultiterable.ResultIterable at 0x7fa8cb72edc0>)]"
     ]
    }
   ],
   "source": [
    "grouped_by_key = partial_count.groupByKey()\n",
    "grouped_by_key.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a89cbdc7-e99e-461e-a43d-53383551b671",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: ('never', <pyspark.resultiterable.ResultIterable at 0x7fa8cbc7d2b0>)"
     ]
    }
   ],
   "source": [
    "# Each element of grouped_by_key is a tuple, and inside a tuple there is an iterable we can iterate over.\n",
    "\n",
    "first_item = grouped_by_key.take(1)[0]\n",
    "first_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e5303f7-b635-4deb-b7d1-2bd04e53e973",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def print_item(item_as_tuple):\n",
    "    token_name, occurences = item_as_tuple\n",
    "    occurences_as_list = list(occurences)\n",
    "    print(f\"{token_name}: {occurences_as_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bbc4fff-66a9-4a7b-ae6f-501bc2a10dec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never: [1, 1, 1, 1]\ncause: [1, 1]\npain: [1]\nonly: [1, 1, 1, 1, 1, 1, 1]\nin: [1, 1]\nrain: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nPurple: [1, 1, 1, 1, 1, 1, 1, 1, 1]\nrain,: [1, 1, 1, 1, 1, 1, 1, 1, 1]\nbathing: [1]\n"
     ]
    }
   ],
   "source": [
    "# Taking the first 10 items from grouped_by_key and then iterating over them.\n",
    "for item in grouped_by_key.take(10):\n",
    "    print_item(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4d96f4-6bfd-4971-bd9a-880c2c3f4ec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: [('never', 4),\n ('cause', 2),\n ('pain', 1),\n ('only', 7),\n ('in', 2),\n ('rain', 14),\n ('', 10),\n ('Purple', 9),\n ('rain,', 9),\n ('bathing', 1)]"
     ]
    }
   ],
   "source": [
    "# When you take the first 10 elements of grouped_by_key, it returns a list of Tuple[str, ResultIterable].\n",
    "# What I want instead is a list of Tuple[str, int] where the second element is the total number of occurence for the fist element.\n",
    "[(token, sum(list(occurences))) for token, occurences in grouped_by_key.take(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b71670-192b-412d-8f82-29520ecc6f62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reduce_function(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0abcd5-78ab-46dd-984b-901997dc59b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: [('never', 4),\n ('cause', 2),\n ('pain', 1),\n ('only', 7),\n ('in', 2),\n ('rain', 14),\n ('', 10),\n ('Purple', 9),\n ('rain,', 9),\n ('bathing', 1)]"
     ]
    }
   ],
   "source": [
    "reduced = partial_count.reduceByKey(reduce_function)\n",
    "reduced.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8544bc0e-af88-456e-a6c3-957db8283f94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: [('rain', 14),\n ('I', 14),\n ('you', 14),\n ('purple', 14),\n ('to', 13),\n ('', 10),\n ('Purple', 9),\n ('rain,', 9),\n ('only', 7),\n ('see', 6)]"
     ]
    }
   ],
   "source": [
    "# I've got a list of tuples, where the key is the token, and the value is its count within the text, but... they're not ordered. Which is inconvenient if I want to have the 10 most popular tokens within the text.\n",
    "sorted_counts = reduced.sortBy(lambda t: t[1])\n",
    "sorted_counts.take(10)\n",
    "desc_sorted_counts = reduced.sortBy(lambda t: -t[1])# descending sort\n",
    "desc_sorted_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c78ac36-dfa6-4d34-a6b0-9f230e5284d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "010e5116-8c81-4c03-811c-433658b50942",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17fd7646-f346-46f3-a467-a24d4c0bac08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d165f0f-87ca-486a-a818-fc3d81e52c63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7ec4da0-deb5-4b3c-b56c-01c6668019e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8691aa10-eeb8-4938-962b-c23c705310bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7737538c-d247-4f46-b622-43182f59cbfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc8f8933-d534-4172-ab9a-418c72b4cc65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "163b856e-6f1e-487a-8782-7cbe6d399bf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-04-28 12:16:21",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
